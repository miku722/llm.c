from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokens = [12,43,27912,12,513,532,21095,33,12,11806,268,9937,6607,764,4600,887,810,318,339,284,1064,262,14522,12,76,17538,5633,705,531,262,717,6512,764,4600,1375,468,645,10282,26765,837,475,318,994,284,12,820,290,3750,284,12,9201,764,679,1244,355,880,1949,284,4929,262,2344,764,705,383,584,8712,837,705,314,466,407,760,837,3729,837,810,673,318,379,1944,837,475,287,1115,12513,422,783,673,481,1282,284,262,6076,284,13502,607,1986,837,355,673,857,790,1227,618,262,8824,318,1336,837,287,1502,326,673,743,1239,1663,1468,4249,34866,992,837,475,743,1464,1394,262,29955,286,6205,764,705,4600,3894,837,705,531,262,717,6512,837,4600,262,6076,318,407,1290,422,994,764,38451,356,467,290,766,703,340,318,673,857,340,5633,705,4600,2561,4420,837,611,345,588,837,705,531,262,584,764,383,6205,3393,12939,284,1061,262,10087,284,262,6076,837,691,734,1243,925,683,34644,1058,717,837,27380,339,1244,307,16039,618,262,10087,1816,837,290,48550,837,27380,339,1244,4425,6504,286,606,837,1201,339,550,407,12098,284,3283,683,1863,523,23994,764,679,373,1165,10032,284,1394,21693,477,1755,837,1865,465,9751,13351,683,422,11029,2128,306,837,290,618,351,262,14555,17577,339,3114,510,284,262,5509,12,4852,837,339,373,9675,284,766,465,2218,6083,19429,991,16039,351,511,6665,739,511,12098,764,679,15063,465,12607,837,290,13488,1566,262,10087,815,923,837,475,484,750,407,2666,262,1295,477,1110,764,1119,45230,546,422,530,5509,284,1194,2045,329,2057,837,477,1110,890,1566,262,6180,837,618,484,1816,736,284,511,1468,583,354,284,3993,764,383,1306,1110,262,976,1517,3022,837,475,319,262,2368,3329,530,6512,531,284,262,584,837,4600,1675,12,820,356,1276,467,284,262,6076,284,766,262,14522,12,76,17538,13502,607,1986,764,705,1119,6150,319,262,5509,10597,19613,2162,788,484,13112,1497,290,1816,3371,262,5366,764,383,1862,582,705,82,2612,4405,351,9751,27380,339,815,4425,6504,286,465,17555,837,475,339,5257,284,1394,262,10087,287,1570,1566,484,757,49264,2402,257,5509,764,383,1862,582,4966,706,606,1566,339,373,2407,19064,290,503,286,8033,837,290,706,1115,1790,24013,262,10087,379,4129,4251,257,1402,1280,2272,287,262,8222,837,319,262,5743,286,543,484,4624,2405,319,262,1353,286,257,1029,5509,764,1649,262,6205,550,9929,1685,606,837,339,2497,326,612,373,257,1598,6076,287,262,3504,286,262,2272,764,679,3332,866,379,262,2366,286,262,5509,2402,543,262,10087,547,49264,837,290,16399,32649,2280,284,644,484,547,2282,284,1123,584,764,4600,383,4252,318,407,866,1865,837,705,531,262,717,6512,2162,4600,356,1276,4043,1865,27416,10597,262,8824,16736,290,262,34827,2058,284,262,6076,764,345,892,673,481,766,326,1862,10087,5586,739,262,5509,5633,705]
decoded = tokenizer.decode(tokens, skip_special_tokens=True)
print("Decoded text:", decoded)
