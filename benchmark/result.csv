| Model Size | LAMBADA (PPL) | LAMBADA (ACC) | CBT-CN (ACC) | CBT-NE (ACC) | WikiText2 (PPL) | PTB (PPL) | enwik8 (BPB) | text8 (BPB) | WikiText103 (PPL) | 1BW (PPL) |
| ---------- | ------------- | ------------- | ------------ | ------------ | --------------- | --------- | ------------ | ----------- | ----------------- | --------- |
| **SOTA**   | 99.8          | 59.23         | 85.7         | 82.3         | 39.14           | 46.54     | 0.99         | 1.08        | 18.3              | **21.8**  |
| 117M       | **35.13**     | 45.99         | 87.65        | 83.4         | 29.41           | 65.85     | 1.16         | 1.17        | 37.50             | 75.20     |
| 345M       | **15.60**     | 55.48         | 92.35        | 87.1         | 22.76           | 47.33     | 1.01         | 1.06        | 26.37             | 55.72     |
| 762M       | **10.87**     | 60.12         | 93.45        | 88.0         | 19.93           | 40.31     | 0.97         | 1.02        | 22.05             | 44.575    |
| 1542M      | **8.63**      | 63.24         | 93.30        | 89.05        | 18.34           | 35.76     | 0.93         | 0.98        | 17.48             | 42.16     |




gpt2实验值
sota参考值来自：https://paperswithcode.com/task/language-modelling
| Model Size | LAMBADA (PPL) | LAMBADA (ACC) | CBT-CN (ACC) | CBT-NE (ACC) | WikiText2 (PPL) | PTB (PPL) | enwik8 (BPB) | text8 (BPB) | WikiText103 (PPL) | 1BW (PPL) |
| ---------- | ------------- | ------------- | ------------ | ------------ | --------------- | --------- | ------------ | ----------- | ----------------- | --------- |
| SOTA       | 1.92          | 89.7          |              |              |                 |           |              |             |                   |           |
| 124M       | 31477.85      | 62.38         |              |              |                 |           |              |             |                   |           |
